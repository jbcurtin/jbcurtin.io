{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing JSON-Data into PostgreSQL\n",
    "\n",
    "Parsing JSON files can be quick and efficient, importing them into PostgreSQL may take some time. In this article, we'll examine how to save compute resource time by pre-processing the JSON format into a CSV format with PSQL datatypes to be imported into PostgreSQL. This method seems to be the quickest way to import data into PostgreSQL\n",
    "\n",
    "Lets begin with grabbing the Yelp Dataset from Kaggle: https://www.kaggle.com/yelp-dataset/yelp-dataset\n",
    "\n",
    "Once the Yelp Dataset is downloaded, move it to a workspace and unzip the files. You'll find a few JSON files\n",
    "\n",
    "```\n",
    "-rw-r--r-- 1 nobody nobody 4809540040  6月 13 05:52 10100_1035793_bundle_archive.zip\n",
    "-rw-r--r-- 1 nobody nobody      41776  3月 26 01:18 Dataset_Agreement.pdf\n",
    "-rw-r--r-- 1 nobody nobody  152898689  3月 26 01:18 yelp_academic_dataset_business.json\n",
    "-rw-r--r-- 1 nobody nobody  449663480  3月 26 01:18 yelp_academic_dataset_checkin.json\n",
    "-rw-r--r-- 1 nobody nobody 6325565224  3月 26 01:19 yelp_academic_dataset_review.json\n",
    "-rw-r--r-- 1 nobody nobody  263489322  3月 26 01:31 yelp_academic_dataset_tip.json\n",
    "-rw-r--r-- 1 nobody nobody 3268069927  3月 26 01:32 yelp_academic_dataset_user.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that our code works here in this notebook. We'll use fake-data in place of the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = '''{\"review_id\":\"xQY8N_XvtGbearJ5X4QryQ\",\"user_id\":\"OwjRMXRC0KyPrIlcjaXeFQ\",\"business_id\":\"-MhfebM0QIsKt87iDN-FNw\",\"stars\":2.0,\"useful\":5,\"funny\":0,\"cool\":0,\"text\":\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\",\"date\":\"2015-04-15 05:21:16\"}\n",
    "{\"review_id\":\"UmFMZ8PyXZTY2QcwzsfQYA\",\"user_id\":\"nIJD_7ZXHq-FX8byPMOkMQ\",\"business_id\":\"lbrU8StCq3yDfr-QMnGrmQ\",\"stars\":1.0,\"useful\":1,\"funny\":1,\"cool\":0,\"text\":\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\", \"date\":\"2013-12-07 03:16:52\"}\n",
    "{\"review_id\":\"LG2ZaYiOgpr2DK_90pYjNw\",\"user_id\":\"V34qejxNsCbcgD8C0HVk-Q\",\"business_id\":\"HQl28KMwrEKHqhFrrDqVNQ\",\"stars\":5.0,\"useful\":1,\"funny\":0,\"cool\":0,\"text\":\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\", \"date\":\"2015-12-05 03:18:11\"}\n",
    "'''\n",
    "fake_data_filepath = 'yelp-fake-data.json'\n",
    "with open(fake_data_filepath, 'w') as stream:\n",
    "  stream.write(fake_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with importing the libraries we'll need to run the codes, setup `logging` and create a few constants near the top of the file. The `logger` probably isn't needed in a Jupyter Notebook, I'll include it though because this post is about transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import csv\n",
    "import hashlib\n",
    "import logging\n",
    "import glob\n",
    "import json\n",
    "import typing\n",
    "import uuid\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "logger = logging.getLogger('')\n",
    "sysHandler = logging.StreamHandler()\n",
    "sysHandler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(sysHandler)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "ENCODING: str = 'utf-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that'll help keep our transformed dataset clean of dumplicate records. `datum_hash` takes in a python-DICT and python-DICT->Keys and returns the hash. The Keys are required to maintain order of the dict->str encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_hash(datum, headers) -> str:\n",
    "    data = ''.join([str(datum[h]) for h in headers])\n",
    "    return hashlib.sha256(data.encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `convert_json_to_csv` function signature takes in a lot. `filepath` converts the `|-.json` path to `|-.csv` and will write the output into the same directory.\n",
    "`unique_together` takes a list of keys which values will be comebined and hashed to create a unique hash. If the hashed-value is already know. It'll be omitted in the CSV output\n",
    "`only_columns` trims down the output value to only the columns requested, or all the columns\n",
    "`mutators` is a list of functions that'll accept and return string values for each propery specified\n",
    "`extenders` is a list of functions that'll accept an array of rows, and extend the rows according to the data-type found in the rows[0]\n",
    "\n",
    "With this, we have a fairly versatile script that'll give us uniform output for each JSON-entry in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_csv(\n",
    "    filepath: str,\n",
    "    unique_together: typing.List[str],\n",
    "    only_columns: typing.List[str] = [],\n",
    "    mutators: typing.Dict['key-name', typing.List['function']] = {},\n",
    "    extenders: typing.Dict['python-type', typing.List['function']] = {}) -> None:\n",
    "\n",
    "    csv_filepath = filepath.rsplit('.', 1)[0]\n",
    "    logger.info(f'Converting file: {csv_filepath} to CSV')\n",
    "    csv_filepath = f'{csv_filepath}.csv'\n",
    "    inserts = {}\n",
    "    def _write_rows(rows: typing.List[typing.List[typing.Any]], writer: 'csv.Writer') -> None:\n",
    "        result = rows[:]\n",
    "        for data_type, extender_list in extenders.items():\n",
    "            for extender in extender_list:\n",
    "                result = extender(result)\n",
    "\n",
    "        for row in result:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    with open(filepath, 'rb') as json_stream:\n",
    "        line = json_stream.readline().decode('utf-8').strip('\\n')\n",
    "        datum = json.loads(line)\n",
    "        with open(csv_filepath, 'w') as csv_stream:\n",
    "            writer = csv.writer(csv_stream)\n",
    "            headers = [h for h in datum.keys()]\n",
    "            for header in only_columns:\n",
    "                assert header in headers\n",
    "\n",
    "            if len(only_columns) > 0:\n",
    "                headers = only_columns\n",
    "\n",
    "            for header in unique_together:\n",
    "                assert header in headers, headers\n",
    "\n",
    "            headers.insert(0, 'created')\n",
    "            headers.insert(0, 'guid')\n",
    "            datum['guid'] = str(uuid.uuid4())\n",
    "            datum['created'] = '2020-06-13 17:09:07.189521+00'\n",
    "            for col_name, mutator_list in mutators.items():\n",
    "                for mutator in mutator_list:\n",
    "                    datum[col_name] = mutator(datum[col_name])\n",
    "\n",
    "            writer.writerow(headers)\n",
    "            _write_rows([[datum[h] for h in headers]], writer)\n",
    "            if len(unique_together) > 0:\n",
    "                inserts[insert_hash(datum, unique_together)] = 1\n",
    "\n",
    "            while True:\n",
    "                line = json_stream.readline().decode('utf-8').strip('\\n')\n",
    "                if line == '':\n",
    "                    break\n",
    "\n",
    "                datum = json.loads(line)\n",
    "                datum['guid'] = str(uuid.uuid4())\n",
    "                datum['created'] = '2020-06-13 17:09:07.189521+00'\n",
    "                for col_name, mutator_list in mutators.items():\n",
    "                    for mutator in mutator_list:\n",
    "                        datum[col_name] = mutator(datum[col_name])\n",
    "\n",
    "                datum_hash = insert_hash(datum, unique_together)\n",
    "                if inserts.get(datum_hash, None) is None and len(unique_together) > 0:\n",
    "                    inserts[datum_hash] = 1\n",
    "                    _write_rows([[datum[h] for h in headers]], writer)\n",
    "                elif len(unique_together) == 0:\n",
    "                    _write_rows([[datum[h] for h in headers]], writer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the two utility functions that'll assist in converting the data from JSON->CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datetime_to_psql(value: str) -> str:\n",
    "    YELP_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "    PSQL_FORMAT = '%Y-%m-%d %H:%M:%S.%f+00'\n",
    "    stamp = datetime.strptime(value, YELP_FORMAT)\n",
    "    return stamp.strftime(PSQL_FORMAT)\n",
    "\n",
    "def extend_uniform_lists(rows: typing.List[typing.List[typing.Any]]) -> typing.List[str]:\n",
    "    result = []\n",
    "    for row in rows:\n",
    "        scalar_values = [value for value in row if not isinstance(value, list)]\n",
    "        list_values = [value for value in row if isinstance(value, list)]\n",
    "        if len(list_values) == 0:\n",
    "            result.append(row)\n",
    "            break\n",
    "            \n",
    "        for idx in range(0, len(list_values[0])):\n",
    "            clone = scalar_values[:]\n",
    "            clone.extend([value[idx] for value in list_values])\n",
    "            result.append(clone)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the conversion function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-14 18:07:31,709 - __main__ - INFO - Converting file: /tmp/yelp-fake-data to CSV\n"
     ]
    }
   ],
   "source": [
    "convert_json_to_csv(\n",
    "  fake_data_filepath,\n",
    "  [],\n",
    "  [],\n",
    "  {'date': [convert_datetime_to_psql]},\n",
    "  {list: [extend_uniform_lists]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we have some output in the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 /tmp/yelp-fake-data.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l /tmp/yelp-fake-data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good! Go ahead an import it into a PSQL service with the following command,\n",
    "```\n",
    "psql> COPY review FROM '/tmp/yelp-fake-data.csv' CSV HEADER;\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
