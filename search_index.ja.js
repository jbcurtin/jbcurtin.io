window.searchIndex = {"fields":["title","body"],"pipeline":["trimmer-ja","stemmer-ja"],"ref":"id","version":"0.9.5","index":{"body":{"root":{"docs":{},"df":0,"パ":{"docs":{},"df":0,"フ":{"docs":{},"df":0,"ォ":{"docs":{},"df":0,"ー":{"docs":{},"df":0,"マ":{"docs":{},"df":0,"ン":{"docs":{},"df":0,"ス":{"docs":{"https://jbcurtin.io/ja/articles/":{"tf":1.0}},"df":1}}}}}}},"分":{"docs":{},"df":0,"析":{"docs":{"https://jbcurtin.io/ja/articles/":{"tf":1.0}},"df":1}},"日":{"docs":{},"df":0,"本":{"docs":{},"df":0,"語":{"docs":{"https://jbcurtin.io/ja/search/":{"tf":1.0}},"df":1}}}}},"title":{"root":{"docs":{},"df":0,"パ":{"docs":{},"df":0,"フ":{"docs":{},"df":0,"ォ":{"docs":{},"df":0,"ー":{"docs":{},"df":0,"マ":{"docs":{},"df":0,"ン":{"docs":{},"df":0,"ス":{"docs":{"https://jbcurtin.io/ja/articles/":{"tf":1.0}},"df":1}}}}}}},"分":{"docs":{},"df":0,"析":{"docs":{"https://jbcurtin.io/ja/articles/":{"tf":1.0}},"df":1}},"日":{"docs":{},"df":0,"本":{"docs":{},"df":0,"語":{"docs":{"https://jbcurtin.io/ja/search/":{"tf":1.0}},"df":1}}}}}},"documentStore":{"save":true,"docs":{"https://jbcurtin.io/ja/":{"body":"","id":"https://jbcurtin.io/ja/","title":""},"https://jbcurtin.io/ja/articles/":{"body":"","id":"https://jbcurtin.io/ja/articles/","title":"パフォーマンス分析"},"https://jbcurtin.io/ja/articles/datetime-benchmarks/":{"body":"Datetime parsing and rendering sometimes require optimization when iterating over a large dataset. Say you have about a couple million rows of timestamps you'd like to parse into a datatype. It could take some time if you use the wrong import or crate. In this article, I'll benchmark what it takes to load a couple million datetime stamps with Python's datetime, Numpy, Chrono, and Time\nWe'll explore different architectural considerations and design patterns to improve the ergonomics of using the different software libraries. Providing you with the ability to drop code into your project with minimal effort.\nThere are some considerations to apply when selecting the right datetime alteration software.\n\nLeap Seconds\nLeap Years\nUS Daylight Savings Timezone Offsets\nNanosecond Support\n\nWhere to apply these considerations is an architectural decision, for example storing all datetime strings in a database should be in UTC, would take care of Leap Seconds &amp; Leap years and avoid having to manage US Daylight Timezone Offset. When rendering a datetime string on a webpage using technologies such as VueJS or ReactJS. We can leverage Typescript|JavaScript to transform those datetime strings from UTC into client-facing timezone aware objects. Client-facing browsers such as Firefox, Chrome, Edge, and Opera know where the user is situated based of their computer system clocks. Therefore, we can pass a UTC datetime string into new Date() and it should render in the correct timezone with the correct offset. Furthermore, we can transform those same UTC datetime strings into Python or Rust and format the objects accordingly in the event we need to render a time-series\nPython's datetime Module\nI find when working with the datetime module, its fast enough to use in a JSON-WebAPI, but to slow when loading, rendering, or generating large amounts of data. Therefore, its very well suited in enterprise projects which would could include like Django, FastAPI, or Flask.\n\nIn the implementation below, there are a series of dunder methods implemented on the DateTimeUTC object. Those methods allow for to transform, subtract, add time to an existing object using timedelta. At the core is the formatting, implemented in a similar way the new Date() object has been implemented in common browsers.\n\nThe format %Y-%m-%dT%H:%M:%S%z provides a datetime string that new Date() can interpret without having to manage timezones in TypeScript.\n\nOkay, great. Now that we know how to use the datetime module. Let's benchmark the performance of it\n\nAdding the timezone to the datetime object increased the duration of each benchmark by almost a whole second.\n\nAs you can see, loading the timestamp is immensely slower. We can extrapolate meaning from these results. If our JSON-WebAPIs are receiving more than, say 500,000 requests per-second. Than this is probably an area we could improve the API performance. Its just a lot of CPU time spent rendering data. We could also consider assuming every datetime string is UTC and drop handling the timezone entirely. I personally wouldn't do that; mostly because I think it is more pythonic to keep timezone intact. Explicit is better than implicit\nNumpy's np.datetime64 module\nnumpy provides datetime64 which is timezone unaware. Meaning, it won't handle UTC, EST, IST, JPN, or GMT. Therefore we'll need to omit the timezone data. This probably improves the performance of the module and is much more ideal for loading data into a database, rending to flat files, or producing parquet files. Like we did with the DateTimeUTC class, make a Timestamp class to encapsulate logic for datetime alterations.\n\nAs you can see, there is a lot going on. Most of the logic is used for datetime alterations and not serialization. We'll run the rendering benchmark with the same series of timestamps, but down to nanosecond support and without a timezone.\n\nThe results show sub-second serialization; lets find the mean.\n\n371548.3 microseconds on average, which comes to about 37.1548 milliseconds on average for each benchmark. Considerably faster than the datetime module. Which is to be expected, numpy is accelerated using c-code, and the API is in python to make data processing much more manageable. In fact it has been suggested to me in the past, that the for/loop might be whats slowing down this code and not the serialization routine numpy is performing changing the datetime from datetime64 to a string.\nLet's go ahead and run a benchmark for loading the datetime string.\n\nLoading the datetime string shows similar sub-second results. Let's again, find the mean.\n\nDefinitely a performance hit here when loading the datetime string into np.datetime64. However, it is still a significant performance increase when its compared to Python's datetime module.\nMoving into Rust, one of the expectations is our code will run at faster speeds. We also have to consider not all Python is the same. Some Python packages ship with c-code to improve performance. For example, take a look at httptools, asyncpg, and uvloop. All developed by MagicStack. Yeah, I'm kind of a fan of the software. With that said, Rust can still be written in a way to run slower than Python.\nChrono\nChrono was the first crate I used to perform datetime alterations in Rust. As a result I have an affinity for chrono more so than some of the other choices. When I design software in Rust, its primarily for machine-to-machine comms and less for Server/Client-Browser comms. With that said, I have rolled a couple Web Stacks and the builtin serialization implementing serde is used extensively throughout my code.\n\nFor each iteration of 2 million timestamps, which generates considerably faster than Python's datetime module. DateTime&lt;UTC&gt; is formatted into a datetime string with nano second precision. I presume the timezone alterations will account for some slowdown, but I won't test for that because if the software can support timezones, those should be included in the benchmark so that the maximum amount of information indicates the correct coordinate it time, regardless of where you are on Earth.\nThe Deserialization will take TS_FORMAT and rebuild the datetime strings into Rust types of DateTime&lt;UTC&gt;.\n\nI was staggered by the results. chrono is running multiple orders of magnitude slower than Python's datetime module and Numpy's datetime64 module.\nA second pair of Serialization / Deserialization benchmarks to see if there is a performance difference using serde.\n\nAn obvious performance increase using serde. I didn't expect these results, and I hope someone more familiar with the software can point out what why serde can serialize and deserialize DateTime&lt;UTC&gt; quicker than using .format.\nTime\nI haven't used time much. and it seems to be used reliably with a number of dependencies I've started pulling from. Therefore it has been pushed into my radar and I'd like to determine if the software is more performant than chrono.\nThe benchmark will be setup similar to Chrono benchmarks. We'll test formatting OffsetDateTime into a datetime string and back. Then I'll implement serde and see if there is a performance gain or loss.\n\ntime::OffsetDateTime API took me some time to get used to, but it seems to be more idiomatic than chrono, datetime, and datetime64. Which I think is great, but at the same time it makes it hard to find the correct syntax for say, the ts_format variable. I have specified that I'd like to have nanosecond precision, but I have no idea how to set that level of precision using OffsetDateTime::now_utc()\n\nWay better results than chrono datetime string formatting. Fairly close to Python's datetime module's ability to serialize datetime objects. An obvious performance improvement, but still on par with datetime and behind datetime64\nA second pair of Serialization / Deserialization benchmarks to see if there is a performance difference using serde.\n\nOnly a performance hit with serialization information and a very minor performance hit when deserialization. Very impressive.\nComparative Results\nModule / SoftwareAverage Serialization DurationAverage Deserialization Duration\nPython's datetime3 seconds10 seconds\nNumpy's datetime64371,548 ms774,868.9 ms\nChrono8 seconds11 seconds\nChrono &amp; Serde6 seconds7 seconds\nTime4 seconds7 seconds\nTime &amp; Serde5 seconds7 seconds\n\nThe comparative results have me wondering if there is a correlation between the datetime64 and time results. Obviously, if you're going to load large amounts of data. Numpy's datetime64 datetime strings are the way to go for now. I wonder how well datetime64 would perform in a Spark runtime.\n","id":"https://jbcurtin.io/ja/articles/datetime-benchmarks/","title":" DateTime (de)Serialization Benchmarks from Python, Numpy, Chrono, and Time"},"https://jbcurtin.io/ja/articles/postgresql-compression-benchmark/":{"body":"By far, my favorite RDBMS is PostgreSQL. Having worked with MySQL, MSSQL, Cassandra, Redis and more; the versatility of PostgreSQL continues to inspire me to write better and more complex SQL. I have a piece of software that reaches out to various news websites following best practices for crawling. Scraping content using provided by the sitemap.xml and following the rules set forth by the robots.txt file. The content of these sites can be anything, but the general approach is to collect as much news as possible to see if I can develop a series of heuristics to provide as technical indicators.\nI've been running the software for about two years now, and a massive PostgreSQL table has been created from the result of it. Today, I'd like to start making regular backups of the data in the table. This article will focus on benchmarking how long it'll take to backup a table using the pg_dump program &amp; Docker.\nStanding on the shoulders of giants, I've found a comprehensive review of algorithms used for compression. I'm not concerned with parallel processing; I'll stick to evaluating the programs as provided. What I'm looking to glean is a definitive and consistent measurement of how long it'll take to export and compress information for the massive PostgreSQL table.\nBenchmark Setup\nBenchmark setup is fairly straightforward. A table called location tracks various metrics about URLs such as crawl_delay, domain of the URL, change_ferq, and lastmod. Properties provided by a sites' sitemap.xml. Which in turn allows for the development of an algorithm to select more relevant pages, yet still allow the crawler to search for archived content while not overburdening the website. The location table will be used to identify the best compression algorithm benchmark for the much larger table\n\nThe database is ran using docker, specifically the postgis:15-3.3-alpine image. Port 5432 is exposed to the host, but we won't use it because it seems that connecting to a port exposed to the host will route traffic through the LAN. Instead we'll export data in the container and compress the output from the exec command to a filepath on the host.\nHow the database has been initalized using Docker\n\nThe compression algorithms to be tested are zstd, gzip, bzip2, lz4, and xz.\nHere is the full script to run the benchmark\n\nBenchmark Results\n\nA core trade off when selecting an optimal compression algorithm is the amount of time taken; relative to the ratio of the file before compression size over after compression size.\n$$ Compression Ratio = {Uncompressed Size\\over{Compressed Size}} $$\nUnderstanding the Benchmark Results\nI'm more interested in the backup running quickly and am willing to accept slightly larger archived files. bzip2 and xz both provided the smallest filesize, but the time to archive took well over twenty minutes for each. ztsd took about a minute and a half, and provided a slightly larger file at 1.6GB in size\nData Stream\nI'm not archiving files. I'm archiving a stream coming from a docker exec command and that has additional overhead to consider. Rather than understanding the overhead, lets produce a metric for how many MB/s of information is being transmitted from the docker exec command. How much data is being piped into the various compression algorithms?\nExporting the same stream to dd for about a minute will tell us enough about how much data is being sent to the various compression algorithms\n\nKnowing an average of 98.4 MB/s is being transmitted, we now have enough information to estimate the compression ratio of each archive.\n$$ Seconds \\times \\text{Average MB/s} \\over {Compressed Size} $$\nAlgorithmMathRatio\nzstd$$ 88 \\times 98.4 \\over {1.6} $$5412.0\ngzip$$ 200 \\times 200 \\over {1.7} $$11576.471\nbzip2$$ 1144 \\times 98.4 \\over {1.3} $$86592.0\nlz4$$ 88 \\times 98.4 \\over {2.5} $$3463.680\nxz$$ 2262 \\times 98.4 \\over {1.3} $$201492.923\n\nCompression Ratios can vary based on a multitude of factors, most notably is the amount of repeated information in the file being archived. For instance, text data often repeats while random data does not. Therefore we'll be able to achieve higher compression ratios on archives with text data (such as a PostgreSQL dump).\nArticle References\n\nhttps://www.rootusers.com/gzip-vs-bzip2-vs-xz-performance-comparison/\nhttps://linuxreviews.org/Comparison_of_Compression_Algorithms\nhttps://en.wikipedia.org/wiki/Data_compression_ratio#Definition\n\n","id":"https://jbcurtin.io/ja/articles/postgresql-compression-benchmark/","title":"Compression Benchmarks for PostgreSQL Archives"},"https://jbcurtin.io/ja/search/":{"body":"","id":"https://jbcurtin.io/ja/search/","title":"Article Search 日本語"}},"docInfo":{"https://jbcurtin.io/ja/":{"body":0,"title":0},"https://jbcurtin.io/ja/articles/":{"body":0,"title":2},"https://jbcurtin.io/ja/articles/datetime-benchmarks/":{"body":0,"title":0},"https://jbcurtin.io/ja/articles/postgresql-compression-benchmark/":{"body":0,"title":0},"https://jbcurtin.io/ja/search/":{"body":0,"title":1}},"length":5},"lang":"Japanese"}